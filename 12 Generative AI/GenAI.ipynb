{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e3eb64-3cdd-4a77-89b2-1f1b2f2bfecd",
   "metadata": {},
   "source": [
    "# Practical Generative AI Course for Beginners\n",
    "\n",
    "**Real-life Examples with Hugging Face and Python**\n",
    "\n",
    "## Course Overview\n",
    "\n",
    "This course is designed for beginners who want to understand and implement generative AI models using Python and the Hugging Face ecosystem. By the end of this course, you'll be able to build and deploy various generative AI applications.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python programming knowledge\n",
    "- Familiarity with pip and virtual environments\n",
    "- Understanding of basic machine learning concepts (helpful but not required)\n",
    "\n",
    "### Module 1: Introduction to Generative AI\n",
    "\n",
    "#### Learning Objectives:\n",
    "\n",
    "- Understand what generative AI is and how it differs from discriminative AI\n",
    "- Learn about key generative AI applications and use cases\n",
    "- Set up your Python environment with necessary libraries\n",
    "\n",
    "#### Practical Example: Text Generation with GPT-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f2b738-ff46-43fc-b61e-1d1e4832bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "#!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532bea1e-c6f3-4e6b-81b8-e55fce7341dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9885505f1d704fe19160a9ebc77230a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\John\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54609903497492da0fa7614a3e5b68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfb0919e5cd4d49a2dff2c8fd2e4f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2abb0561b841b382cbb86a00c4fdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1425e69b137e447a99b032e089c49f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463f536c40cf45d3953394321ee40a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\John\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence will be able to do things like search for and find people, and to do things like find out who's in the right place at the right time.\n",
      "\n",
      "\"We're going to be able to do things like that, and\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "# pip install transformers torch\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Artificial intelligence will\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d93f6-3e5e-4340-8361-5eab1ef8ddfc",
   "metadata": {},
   "source": [
    "### Module 2: Working with Transformers and Hugging Face\n",
    "\n",
    "#### Learning Objectives:\n",
    "\n",
    "- Understand transformer architecture fundamentals\n",
    "- Navigate the Hugging Face Hub and ecosystem\n",
    "- Learn to fine-tune pre-trained models for specific tasks\n",
    "\n",
    "#### Practical Example: Sentiment Analysis with DistilBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9462d9c9-cef7-4948-9da6-cb5c73883eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7309c59bb744ed97e96a67629adecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\John\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9ce1761b794ee1bd5528788b0dfeb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5f7f430ffb4f54bfa5520bf74a4ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51a56f661d64f0aa45cf5be157a9b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I absolutely loved this product! It exceeded all my expectations.\n",
      "Sentiment: POSITIVE, Score: 0.9999\n",
      "\n",
      "Text: The service was terrible and the staff was rude.\n",
      "Sentiment: NEGATIVE, Score: 0.9997\n",
      "\n",
      "Text: It was okay, not great but not bad either.\n",
      "Sentiment: POSITIVE, Score: 0.9977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers datasets torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained model for sentiment analysis\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Create a pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Analyze sentiments\n",
    "texts = [\n",
    "    \"I absolutely loved this product! It exceeded all my expectations.\",\n",
    "    \"The service was terrible and the staff was rude.\",\n",
    "    \"It was okay, not great but not bad either.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result = sentiment_analyzer(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result[0]['label']}, Score: {result[0]['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62304d-0a37-4a9a-88b4-80419a7ef3aa",
   "metadata": {},
   "source": [
    "### Module 3: Image Generation with Diffusion Models\n",
    "\n",
    "#### Learning Objectives:\n",
    "\n",
    "- Understand diffusion models and their applications\n",
    "- Learn to use Stable Diffusion via Hugging Face\n",
    "- Implement text-to-image generation\n",
    "\n",
    "#### Practical Example: Text-to-Image Generation with Stable Diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25d9f062-f120-4d8b-acf2-ab58133e0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install diffusers transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8528939-3e79-401d-bfde-0e6ac9f5b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pip install diffusers transformers accelerate torch\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Replace with your actual Hugging Face token\n",
    "hf_token = \"YOUR_HUGGINGFACE_TOKEN\"\n",
    "\n",
    "# Load the model (requires authentication for some models)\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")  # Use \"cpu\" if you don't have a GPU\n",
    "\n",
    "# Generate an image from a text prompt\n",
    "prompt = \"A serene landscape with mountains and a lake at sunset, digital art\"\n",
    "image = pipe(prompt).images[0]\n",
    "\n",
    "# Save the generated image\n",
    "image.save(\"generated_landscape.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff06164-ba03-48e9-8d64-40e3aa26df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image if in notebook\n",
    "from IPython.display import display\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc7d60-7450-4c83-953d-5645be4d0c17",
   "metadata": {},
   "source": [
    "### Module 4: Fine-tuning Language Models\n",
    "\n",
    "#### Learning Objectives:\n",
    "\n",
    "- Understand the concept of fine-tuning pre-trained models\n",
    "- Learn techniques for efficient fine-tuning (LoRA, adapters)\n",
    "- Fine-tune a language model on custom data\n",
    "\n",
    "#### Practical Example: Fine-tuning GPT-2 for Custom Text Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a63cd-f878-4503-9b73-39376a6e3766",
   "metadata": {},
   "source": [
    "### Explanatory Notes for GPT-2 Fine-tuning Code\n",
    "\n",
    "#### Overall Structure and Purpose\n",
    "\n",
    "This code demonstrates how to fine-tune a pre-trained GPT-2 language model on a custom dataset for improved text generation. It's organized into three main functions:\n",
    "\n",
    "- **`fine_tune_text_generator()`:** Handles the model fine-tuning process\n",
    "- **`generate_text()`:** Uses the fine-tuned model to generate new text\n",
    "- **`main()`:** Orchestrates the workflow\n",
    "\n",
    "#### Key Components Explained\n",
    "\n",
    "- **Model Setup**\n",
    "  - The code loads a pre-trained GPT-2 model and tokenizer from Hugging Face\n",
    "  - It checks for GPU availability and moves the model to the appropriate device\n",
    "  - Since GPT-2 doesn't have a built-in padding token, it uses the end-of-sequence token as padding\n",
    "\n",
    "- **Dataset Preparation**\n",
    "  - Loads the WikiText dataset (specifically `wikitext-2-raw-v1`)\n",
    "  - Tokenizes the text data with appropriate parameters:\n",
    "    - Truncates texts to 512 tokens maximum\n",
    "    - Adds padding\n",
    "    - Includes attention masks\n",
    "  - For demonstration purposes, only 1000 examples are used from the training set\n",
    "\n",
    "- **Training Configuration**\n",
    "  - Uses `DataCollatorForLanguageModeling` for causal (next token prediction) language modeling\n",
    "  - Sets up training arguments including:\n",
    "    - Number of epochs (default 3)\n",
    "    - Batch size (default 8)\n",
    "    - Logging and checkpoint saving frequencies\n",
    "  - The `Trainer` class handles the training loop, optimization, and logging\n",
    "\n",
    "- **Text Generation**\n",
    "  - The `generate_text()` function creates new text from prompts using the fine-tuned model\n",
    "  - Generation parameters control the output:\n",
    "    - `temperature:` Controls randomness (0.8 provides a good balance)\n",
    "    - `top_k` and `top_p:` Limit token selection to improve quality\n",
    "    - `no_repeat_ngram_size:` Prevents exact repetition of phrases\n",
    "    - `num_return_sequences:` Creates multiple outputs from the same prompt\n",
    "\n",
    "- **Workflow in `main()`**\n",
    "  - Fine-tunes the model (with an option to load a previously fine-tuned model)\n",
    "  - Demonstrates generation with three different prompts\n",
    "  - Outputs multiple samples for each prompt\n",
    "\n",
    "#### Implementation Notes\n",
    "\n",
    "- The code is flexible, allowing customization of dataset, model size, and training parameters\n",
    "- It demonstrates proper handling of devices (CPU/GPU)\n",
    "- For efficiency, it includes options to skip re-training if you've already fine-tuned a model\n",
    "- The generation showcases how to control text generation parameters for different creative outputs\n",
    "\n",
    "This is a complete pipeline for fine-tuning and using GPT-2 for text generation, following best practices from the Hugging Face ecosystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0adfa04-3b0b-4102-94d2-1d9ef6114a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   use GOOGLE COLAB GPU \n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, \n",
    "    GPT2LMHeadModel,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "def fine_tune_text_generator(\n",
    "    dataset_name=\"wikitext\",\n",
    "    dataset_config=\"wikitext-2-raw-v1\",\n",
    "    model_name=\"gpt2\",\n",
    "    output_dir=\"./fine-tuned-gpt2\",\n",
    "    num_epochs=3,\n",
    "    batch_size=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune a GPT-2 model on a text dataset for improved text generation.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset to use from Hugging Face datasets\n",
    "        dataset_config: Specific configuration of the dataset\n",
    "        model_name: Pre-trained model to fine-tune\n",
    "        output_dir: Directory to save the fine-tuned model\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    print(f\"Loading {model_name} model and tokenizer...\")\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Add padding token (needed for batched training)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    print(f\"Loading {dataset_name} dataset...\")\n",
    "    # Load and prepare dataset\n",
    "    dataset = load_dataset(dataset_name, dataset_config)\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True  # Explicitly request attention mask\n",
    "        )\n",
    "    \n",
    "    print(\"Tokenizing dataset...\")\n",
    "    # Apply tokenization to dataset\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    # Use only a small subset for this example\n",
    "    train_dataset = tokenized_dataset[\"train\"].select(range(1000))\n",
    "    \n",
    "    # Data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # We're doing causal language modeling, not masked\n",
    "    )\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=100,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "    )\n",
    "    \n",
    "    # Set up trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Saving model to {output_dir}...\")\n",
    "    # Save model\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=\"Once upon a time\",\n",
    "    max_length=100,\n",
    "    num_samples=3,\n",
    "    temperature=0.8\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text using a fine-tuned GPT-2 model.\n",
    "    \n",
    "    Args:\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        prompt: Starting text for generation\n",
    "        max_length: Maximum length of generated text\n",
    "        num_samples: Number of different samples to generate\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "    \n",
    "    Returns:\n",
    "        List of generated text samples\n",
    "    \"\"\"\n",
    "    # Determine device\n",
    "    device = model.device\n",
    "    \n",
    "    # Encode prompt with attention mask\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_samples,\n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return generated texts\n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "def main():\n",
    "    # Check if GPU is available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Fine-tune model (comment out to skip fine-tuning if already done)\n",
    "    model, tokenizer = fine_tune_text_generator(num_epochs=1)  # Reduced epochs for demo\n",
    "    \n",
    "    # Or load an already fine-tuned model\n",
    "    # model_path = \"./fine-tuned-gpt2\"\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    # model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    # model = model.to(device)  # Make sure to move loaded model to the correct device\n",
    "    \n",
    "    # Generate text with different prompts\n",
    "    prompts = [\n",
    "        \"The future of artificial intelligence\",\n",
    "        \"Climate change poses\",\n",
    "        \"In the world of quantum computing\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n\\nPrompt: {prompt}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        generated_texts = generate_text(model, tokenizer, prompt=prompt)\n",
    "        \n",
    "        for i, text in enumerate(generated_texts, 1):\n",
    "            print(f\"Sample {i}:\")\n",
    "            print(text)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec4878-5c30-45e3-a399-61ba4cd10be0",
   "metadata": {},
   "source": [
    "### Module 5: Building Conversational AI with Hugging Face\n",
    "\n",
    "#### Learning Objectives:\n",
    "\n",
    "- Understand dialog systems architecture\n",
    "- Implement a simple chatbot using pre-trained models\n",
    "- Learn about retrieval-augmented generation (RAG)\n",
    "\n",
    "#### Practical Example: Simple Chatbot with Blenderbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a7759-15fa-4341-b210-1cac8697d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch\n",
    "\n",
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(input_text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate response\n",
    "    response_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Simple conversation loop\n",
    "def chat():\n",
    "    print(\"Bot: Hello! I'm a chatbot. Type 'exit' to end our conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "        response = generate_response(user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "# Run the chatbot\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6f2ea-e261-4b48-be9e-5e82dbdb9243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "660a2217-df2a-4f47-b597-fc66ca6e1fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9ea37859a645348a93b3f7fc241730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\John\\.cache\\huggingface\\hub\\models--Salesforce--blip-image-captioning-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ac727f4c0849c7a0d1905413c794d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3478c2dfa8fb43cdbcab958d2da8f4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84f97f6bb2f4daca9c6492dd6ecb01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd2c6c715c14b278274b22daf52bc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded567b4b84c4053a1672dc8e7d44a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b46b17b7b04888893ae4ed0f4dc007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8443259044e4c9888b0c1d678da3dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: two cats sleeping on a couch\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers pillow torch\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Function to generate captions for images\n",
    "def generate_caption(image_path_or_url):\n",
    "    # Load image\n",
    "    if image_path_or_url.startswith('http'):\n",
    "        image = Image.open(requests.get(image_path_or_url, stream=True).raw)\n",
    "    else:\n",
    "        image = Image.open(image_path_or_url)\n",
    "    \n",
    "    # Process image and generate caption\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs, max_length=30)\n",
    "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Example usage\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "caption = generate_caption(image_url)\n",
    "print(f\"Generated caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f3b02-ac09-4433-a3b7-7920f9f12ec9",
   "metadata": {},
   "source": [
    "### Module 7: Deploying Generative AI Models\n",
    "\n",
    "#### Learning Objectives:\n",
    "\n",
    "- Learn strategies for optimizing and deploying generative AI models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99be3c8-75d0-4c40-b74e-3d821c04670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"AI Text Generator\",\n",
    "    page_icon=\"✨\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Custom CSS for better styling\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main {\n",
    "        padding: 1rem 1rem;\n",
    "    }\n",
    "    .title {\n",
    "        font-size: 2.5rem;\n",
    "        font-weight: bold;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "    .subtitle {\n",
    "        font-size: 1.2rem;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .stTextInput>div>div>input {\n",
    "        padding: 0.5rem;\n",
    "        font-size: 1rem;\n",
    "    }\n",
    "    .output-container {\n",
    "        background-color: #f0f2f6;\n",
    "        border-radius: 0.5rem;\n",
    "        padding: 1.5rem;\n",
    "        margin-top: 1rem;\n",
    "    }\n",
    "    .params-section {\n",
    "        background-color: #f8f9fa;\n",
    "        border-radius: 0.5rem;\n",
    "        padding: 1rem;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model(model_name=\"gpt2\"):\n",
    "    \"\"\"Load the model and tokenizer (cached to avoid reloading)\"\"\"\n",
    "    try:\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        return pipeline('text-generation', model=model_name, device=device)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Header\n",
    "    st.markdown('<p class=\"title\">AI Text Generator</p>', unsafe_allow_html=True)\n",
    "    st.markdown('<p class=\"subtitle\">Generate creative text using the power of GPT-2!</p>', unsafe_allow_html=True)\n",
    "    \n",
    "    # Sidebar for model selection\n",
    "    with st.sidebar:\n",
    "        st.header(\"Model Settings\")\n",
    "        model_option = st.selectbox(\n",
    "            \"Select model\",\n",
    "            options=[\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"distilgpt2\"],\n",
    "            index=0,\n",
    "            help=\"Larger models provide better quality but take longer to generate\"\n",
    "        )\n",
    "        \n",
    "        st.subheader(\"About\")\n",
    "        st.markdown(\"\"\"\n",
    "        This app uses Hugging Face's transformers library to generate text with GPT-2 models.\n",
    "        \n",
    "        - **gpt2**: 124M parameters (fastest)\n",
    "        - **distilgpt2**: 82M parameters (smaller, slightly lower quality)\n",
    "        - **gpt2-medium**: 355M parameters (better quality, slower)\n",
    "        - **gpt2-large**: 774M parameters (best quality, slowest)\n",
    "        \"\"\")\n",
    "    \n",
    "    # Load the model\n",
    "    with st.spinner(\"Loading the model... This might take a moment.\"):\n",
    "        generator = load_model(model_option)\n",
    "    \n",
    "    if not generator:\n",
    "        st.warning(\"Failed to load the model. Please try again.\")\n",
    "        return\n",
    "        \n",
    "    # Input section\n",
    "    st.header(\"Enter your prompt\")\n",
    "    prompt = st.text_area(\n",
    "        \"Type a starting prompt for the AI to continue\",\n",
    "        value=\"Once upon a time in a land far away,\",\n",
    "        height=100,\n",
    "        max_chars=500,\n",
    "        help=\"The AI will continue from this text\"\n",
    "    )\n",
    "    \n",
    "    # Parameters section\n",
    "    with st.expander(\"Advanced Parameters\", expanded=False):\n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            max_length = st.slider(\n",
    "                \"Maximum length\",\n",
    "                min_value=10,\n",
    "                max_value=512,\n",
    "                value=150,\n",
    "                step=10,\n",
    "                help=\"Maximum number of tokens to generate\"\n",
    "            )\n",
    "            \n",
    "            num_sequences = st.slider(\n",
    "                \"Number of sequences\",\n",
    "                min_value=1,\n",
    "                max_value=5,\n",
    "                value=1,\n",
    "                help=\"Number of different completions to generate\"\n",
    "            )\n",
    "        \n",
    "        with col2:\n",
    "            temperature = st.slider(\n",
    "                \"Temperature\",\n",
    "                min_value=0.1,\n",
    "                max_value=1.5,\n",
    "                value=0.7,\n",
    "                step=0.1,\n",
    "                help=\"Higher values increase randomness, lower values make output more deterministic\"\n",
    "            )\n",
    "            \n",
    "            top_p = st.slider(\n",
    "                \"Top-p (nucleus sampling)\",\n",
    "                min_value=0.1,\n",
    "                max_value=1.0,\n",
    "                value=0.9,\n",
    "                step=0.05,\n",
    "                help=\"Controls diversity via nucleus sampling\"\n",
    "            )\n",
    "    \n",
    "    # Generation button\n",
    "    if st.button(\"Generate Text\", type=\"primary\"):\n",
    "        with st.spinner(\"Generating text... Please wait.\"):\n",
    "            try:\n",
    "                # Track generation time\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Generate text\n",
    "                results = generator(\n",
    "                    prompt,\n",
    "                    max_length=max_length,\n",
    "                    num_return_sequences=num_sequences,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=True,\n",
    "                    no_repeat_ngram_size=2\n",
    "                )\n",
    "                \n",
    "                # Calculate generation time\n",
    "                generation_time = time.time() - start_time\n",
    "                \n",
    "                # Display results\n",
    "                st.success(f\"Text generated successfully in {generation_time:.2f} seconds!\")\n",
    "                \n",
    "                for i, result in enumerate(results):\n",
    "                    generated_text = result['generated_text']\n",
    "                    \n",
    "                    # Split into prompt and new text for highlighting\n",
    "                    new_text = generated_text[len(prompt):]\n",
    "                    \n",
    "                    with st.container():\n",
    "                        st.markdown(\"### Result \" + (\"\" if num_sequences == 1 else f\"{i+1}\"))\n",
    "                        st.markdown('<div class=\"output-container\">', unsafe_allow_html=True)\n",
    "                        \n",
    "                        # Display with the prompt and generated text in different styles\n",
    "                        st.markdown(f\"**Prompt:** {prompt}\", unsafe_allow_html=True)\n",
    "                        st.markdown(f\"**Generated:** {new_text}\", unsafe_allow_html=True)\n",
    "                        \n",
    "                        # Add copy button\n",
    "                        st.text_area(\n",
    "                            \"Full generated text (copy from here)\",\n",
    "                            value=generated_text,\n",
    "                            height=100,\n",
    "                            label_visibility=\"collapsed\"\n",
    "                        )\n",
    "                        st.markdown('</div>', unsafe_allow_html=True)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                st.error(f\"An error occurred during text generation: {str(e)}\")\n",
    "    \n",
    "    # Information section at the bottom\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"\"\"\n",
    "    ### Tips for better results:\n",
    "    \n",
    "    - Provide a detailed and clear prompt\n",
    "    - Adjust temperature to control randomness (higher = more creative but potentially less coherent)\n",
    "    - Experiment with different models for different quality levels\n",
    "    - For coherent stories, use a lower temperature (0.3-0.7)\n",
    "    - For creative ideas or brainstorming, use a higher temperature (0.7-1.0)\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
