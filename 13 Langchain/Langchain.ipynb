{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e0fe6e-96db-418b-9868-d2a413550859",
   "metadata": {},
   "source": [
    "# Practical LangChain Course for Beginners: Real-life Examples (Python)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Introduction to LangChain\n",
    "- Setting Up Your Environment\n",
    "- LangChain Core Concepts\n",
    "- Working with LLMs\n",
    "- Prompt Engineering with LangChain\n",
    "- Building Conversational Agents\n",
    "- Document Processing and Retrieval\n",
    "- Real-life Example: Building a Personal Knowledge Base\n",
    "- Real-life Example: Creating a Customer Support Bot\n",
    "- Real-life Example: Data Analysis Assistant\n",
    "- Troubleshooting and Best Practices\n",
    "- Next Steps and Advanced Concepts\n",
    "\n",
    "## Introduction to LangChain\n",
    "\n",
    "### What is LangChain?\n",
    "\n",
    "LangChain is a framework designed to simplify the development of applications powered by language models. It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
    "\n",
    "### Why Use LangChain?\n",
    "\n",
    "- **Unified Interface:** Consistent ways to interact with different language models\n",
    "- **Components:** Reusable modules for working with language models\n",
    "- **Pre-built Chains:** Standard implementations for common use cases\n",
    "- **Integration:** Easy connection with external data sources and tools\n",
    "\n",
    "### Evolution and Current State\n",
    "\n",
    "LangChain has evolved from a simple tool for chaining prompts to a robust framework for building complex LLM applications. The latest version emphasizes composability, modularity, and integration with various tools and data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8c4a9-a352-4ff7-8361-663c8b24e322",
   "metadata": {},
   "source": [
    "### Setting Up Your Environment\n",
    "Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31f842c9-5e6d-4e9d-b2d1-1bf0f233b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a virtual environment (recommended)\n",
    "#python -m venv langchain-env\n",
    "#source langchain-env/bin/activate  # On Windows, use: langchain-env\\Scripts\\activate\n",
    "\n",
    "# Install LangChain and related packages\n",
    "#! pip install langchain openai tiktoken unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0af48372-fd42-4f19-9561-cdc7e889eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d536e3-6bcd-4daa-907b-1d7a3220d1d0",
   "metadata": {},
   "source": [
    "### API Key Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e3c53a5-897f-4f67-95d1-d795943f8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb6234e1-b4e0-4a81-8455-86a763cd65a7",
   "metadata": {},
   "source": [
    "Create a .env file in your project directory:\n",
    "\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "SERPAPI_API_KEY=your_serpapi_api_key  # If using web search capabilities\n",
    "PINECONE_API_KEY=your_pinecone_api_key  # If using Pinecone for vector storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e705a7f-09e2-4bad-8c9c-b1512884cd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set up API keys (best to use environment variables)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set. Check your .env file.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e29a10-ef0f-479e-8d0b-f2e48ddff891",
   "metadata": {},
   "source": [
    "### Testing Your Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6e974-43dc-497f-961b-9648a60d0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic example of using LangChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "# Create a chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "result = chain.run(\"eco-friendly water bottles\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548ae1b-1e56-4daf-9584-b5db02e439b6",
   "metadata": {},
   "source": [
    "### LangChain Core Concepts\n",
    "\n",
    "#### Components Architecture\n",
    "\n",
    "LangChain's modular architecture consists of several key components:\n",
    "\n",
    "- **Models:** Wrappers around LLMs (OpenAI, Anthropic, etc.)\n",
    "- **Prompts:** Templates and management for model inputs\n",
    "- **Indexes:** Tools for structuring document data\n",
    "- **Chains:** Sequences of operations for specific tasks\n",
    "- **Agents:** Dynamic chains that use LLMs to determine actions\n",
    "- **Memory:** State retention between chain runs\n",
    "\n",
    "#### Data Connections\n",
    "\n",
    "LangChain provides connectors for various data sources:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc74ba-3ed3-45d8-a875-c40541825f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading data from a text file\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./data/sample_text.txt\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Preview: {documents[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ceff8f-e875-46c4-80aa-f505ee9f1fb7",
   "metadata": {},
   "source": [
    "#### Understanding Chains\n",
    "\n",
    "Chains combine multiple components to create end-to-end applications:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22939a03-e3bc-43bc-8917-c0ca62dcadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First chain: Generate a product idea\n",
    "llm = OpenAI(temperature=0.7)\n",
    "prompt_template_1 = PromptTemplate(\n",
    "    input_variables=[\"subject\"],\n",
    "    template=\"Generate an innovative product idea related to {subject}.\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_template_1)\n",
    "\n",
    "# Second chain: Generate marketing copy for the product\n",
    "prompt_template_2 = PromptTemplate(\n",
    "    input_variables=[\"product_idea\"],\n",
    "    template=\"Write a short marketing description for this product: {product_idea}\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_template_2)\n",
    "\n",
    "# Combine chains in a sequence\n",
    "overall_chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
    "\n",
    "# Run the chain\n",
    "result = overall_chain.run(\"renewable energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca17b53-0d14-4f74-9612-0cc8df79a8f3",
   "metadata": {},
   "source": [
    "### Working with LLMs\n",
    "\n",
    "#### Supported Models\n",
    "\n",
    "LangChain supports various LLM providers:\n",
    "\n",
    "- OpenAI (GPT models)\n",
    "- Anthropic (Claude)\n",
    "- Hugging Face models\n",
    "- Local LLMs (via APIs like LlamaCpp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e56beca-f70b-44fa-baf9-54ca3112bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenAI\n",
    "from langchain.llms import OpenAI\n",
    "openai_llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Using Anthropic\n",
    "from langchain.llms import Anthropic\n",
    "anthropic_llm = Anthropic(model=\"claude-2\", temperature=0)\n",
    "\n",
    "# Using a Hugging Face model\n",
    "from langchain.llms import HuggingFaceHub\n",
    "huggingface_llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xl\", \n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0d4d5-c84e-4077-8e2e-31276e28f2b0",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "Understanding key parameters for controlling LLM behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0c7c3-47f8-4192-9e9c-411aad7f67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Temperature controls randomness (0 = deterministic, 1 = creative)\n",
    "deterministic_llm = OpenAI(temperature=0)\n",
    "creative_llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# Max tokens controls response length\n",
    "concise_llm = OpenAI(max_tokens=50)\n",
    "detailed_llm = OpenAI(max_tokens=500)\n",
    "\n",
    "# Examples\n",
    "prompt = \"Explain quantum computing\"\n",
    "print(\"Deterministic:\", deterministic_llm(prompt))\n",
    "print(\"Creative:\", creative_llm(prompt))\n",
    "print(\"Concise:\", concise_llm(prompt))\n",
    "print(\"Detailed:\", detailed_llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd6218-4299-498d-a05f-b43a97726397",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "For better user experience with longer outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4b800-f762-40ae-9b90-3fdcaae5cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Initialize LLM with streaming\n",
    "llm = OpenAI(\n",
    "    streaming=True, \n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# This will stream the response token by token\n",
    "llm(\"Write a short story about a robot learning to paint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d8d17-e0e8-4222-bf2a-3840dde9e118",
   "metadata": {},
   "source": [
    "### Prompt Engineering with LangChain\n",
    "- Prompt Templates\n",
    "Creating reusable, parameterized prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f119a-53b0-4713-b648-c4ab6d82e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Simple template with one variable\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Provide three interesting facts about {topic}.\"\n",
    ")\n",
    "\n",
    "# Generate a prompt\n",
    "prompt = template.format(topic=\"deep sea creatures\")\n",
    "print(prompt)\n",
    "\n",
    "# Multiple variables\n",
    "multi_template = PromptTemplate(\n",
    "    input_variables=[\"product\", \"audience\", \"tone\"],\n",
    "    template=\"Write a {tone} advertisement for {product} targeted at {audience}.\"\n",
    ")\n",
    "\n",
    "ad_prompt = multi_template.format(\n",
    "    product=\"smart water bottle\",\n",
    "    audience=\"fitness enthusiasts\",\n",
    "    tone=\"energetic\"\n",
    ")\n",
    "print(ad_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162a71-a4da-424c-ad25-70f23199ffc0",
   "metadata": {},
   "source": [
    "### Few-Shot Learning\n",
    "Using examples to guide model responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964c73f-5919-4983-a870-2dafefef31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Define example formatter\n",
    "example_formatter_template = \"\"\"\n",
    "Input: {query}\n",
    "Output: {answer}\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_formatter_template\n",
    ")\n",
    "\n",
    "# Define examples\n",
    "examples = [\n",
    "    {\"query\": \"What is the capital of France?\", \"answer\": \"The capital of France is Paris.\"},\n",
    "    {\"query\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare wrote Romeo and Juliet.\"},\n",
    "    {\"query\": \"What is the boiling point of water?\", \"answer\": \"The boiling point of water is 100 degrees Celsius.\"}\n",
    "]\n",
    "\n",
    "# Create the few-shot prompt template\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Answer the following questions based on these examples:\",\n",
    "    suffix=\"Input: {query}\\nOutput:\",\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Format the prompt with our query\n",
    "query = \"Who painted the Mona Lisa?\"\n",
    "print(few_shot_prompt.format(query=query))\n",
    "\n",
    "# Use with an LLM\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "response = llm(few_shot_prompt.format(query=query))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e4248-e765-469b-99f1-094e54cc23ae",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "Structuring LLM outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a5f26-6a90-4476-8aa8-677cbbb74b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define the structure we want\n",
    "class Movie(BaseModel):\n",
    "    title: str = Field(description=\"Title of the movie\")\n",
    "    director: str = Field(description=\"Director of the movie\")\n",
    "    year: int = Field(description=\"Year the movie was released\")\n",
    "    genres: List[str] = Field(description=\"Genres of the movie\")\n",
    "\n",
    "# Create a parser\n",
    "parser = PydanticOutputParser(pydantic_object=Movie)\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"\n",
    "Provide information about a movie based on this description: {description}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"description\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Format the prompt\n",
    "input_text = prompt.format(description=\"A sci-fi film directed by Christopher Nolan featuring dreams within dreams\")\n",
    "\n",
    "# Get the LLM response\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "output = llm(input_text)\n",
    "\n",
    "# Parse the output\n",
    "movie = parser.parse(output)\n",
    "print(f\"Title: {movie.title}\")\n",
    "print(f\"Director: {movie.director}\")\n",
    "print(f\"Year: {movie.year}\")\n",
    "print(f\"Genres: {', '.join(movie.genres)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79302bf-164d-4a7c-a2f9-d237fc0e8aab",
   "metadata": {},
   "source": [
    "### Building Conversational Agents\n",
    "#### Implementing Memory\n",
    "Adding context retention to conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04efd48-59b8-413c-b68a-e3c572ed936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize conversation with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# First interaction\n",
    "response1 = conversation.predict(input=\"Hi there! My name is Alice.\")\n",
    "print(response1)\n",
    "\n",
    "# Second interaction (the model remembers the name)\n",
    "response2 = conversation.predict(input=\"What's my name?\")\n",
    "print(response2)\n",
    "\n",
    "# Check what's in memory\n",
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f0b78-7e0f-4736-ae9c-ccdfe63904ce",
   "metadata": {},
   "source": [
    "### Different Memory Types\n",
    "#### LangChain offers various memory implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98126adb-37d5-47a2-9c25-20c755e573df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer Memory (stores all interactions)\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "\n",
    "# Summary Memory (stores a summary of conversation)\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.llms import OpenAI\n",
    "summary_memory = ConversationSummaryMemory(llm=OpenAI())\n",
    "\n",
    "# Entity Memory (remembers specific entities)\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "entity_memory = ConversationEntityMemory(llm=OpenAI())\n",
    "\n",
    "# Example with entity memory\n",
    "from langchain.chains import ConversationChain\n",
    "conversation = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=entity_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response1 = conversation.predict(input=\"My favorite food is pizza\")\n",
    "response2 = conversation.predict(input=\"I have a pet dog named Max\")\n",
    "response3 = conversation.predict(input=\"What's my favorite food? And what's my pet's name?\")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708cc309-f8fe-43b4-807c-3dcc559e55d2",
   "metadata": {},
   "source": [
    "### Tools and Agents\n",
    "Enabling LLMs to use tools and make decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62881bc-b0fa-449a-bc49-7b6562c94921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Load tools (requires SERPAPI_API_KEY for search)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=OpenAI(temperature=0))\n",
    "\n",
    "# Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    OpenAI(temperature=0), \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.run(\"What was the highest grossing movie of 2022, and what is the square root of its worldwide box office gross in billions?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4826e841-2bf5-4f35-8974-6d2878891599",
   "metadata": {},
   "source": [
    "### Document Processing and Retrieval\n",
    "#### Document Loading\n",
    "Loading documents from various sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80152e40-0fef-477a-9e50-3158c2f7c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text files\n",
    "from langchain.document_loaders import TextLoader\n",
    "text_loader = TextLoader(\"./data/sample.txt\")\n",
    "text_docs = text_loader.load()\n",
    "\n",
    "# PDFs\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader(\"./data/sample.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# Web pages\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "web_loader = WebBaseLoader(\"https://www.example.com\")\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "# CSV files\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "csv_loader = CSVLoader(\"./data/sample.csv\")\n",
    "csv_docs = csv_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75ebd9-3f54-416f-bb03-53852d80ec13",
   "metadata": {},
   "source": [
    "#### Document Splitting\n",
    "Breaking documents into manageable chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2340575-079d-400a-8042-f8cba99a9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Number of characters per chunk\n",
    "    chunk_overlap=200,  # Overlap between chunks to maintain context\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Load a document\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"./data/long_document.txt\")\n",
    "document = loader.load()[0]\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = text_splitter.split_documents([document])\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "# Preview the first chunk\n",
    "print(f\"First chunk: {chunks[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee716e3-c330-4372-8d95-6f712fbcfc59",
   "metadata": {},
   "source": [
    "#### Vector Stores\n",
    "Creating searchable document embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0497768-e039-42c4-b9ce-80233318bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load document\n",
    "loader = TextLoader(\"./data/state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and store in vector database\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# Perform a similarity search\n",
    "query = \"What did the president say about healthcare?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8156a-40e7-4823-9281-78da33fa4811",
   "metadata": {},
   "source": [
    "#### Retrieval QA Chains\n",
    "Combining document retrieval with LLM question answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b4240-9b1a-4823-89c4-097d4f07fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# Load document and create an index\n",
    "loader = TextLoader(\"./data/company_info.txt\")\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "# Create a question-answering chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Ask questions\n",
    "query = \"What is the company's mission statement?\"\n",
    "response = qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0cb35a-ff22-465f-8e14-593eb7c5cfa4",
   "metadata": {},
   "source": [
    "### Real-life Example: Building a Personal Knowledge Base\n",
    "Let's build a personal knowledge base that can process multiple document types and answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a454eb1-3131-433e-8060-89225a962838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "def build_knowledge_base(directory_path):\n",
    "    # List to store all documents\n",
    "    all_documents = []\n",
    "    \n",
    "    # Process all files in the directory\n",
    "    for file in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        \n",
    "        # Skip directories\n",
    "        if os.path.isdir(file_path):\n",
    "            continue\n",
    "            \n",
    "        # Process based on file extension\n",
    "        if file.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path)\n",
    "            all_documents.extend(loader.load())\n",
    "        elif file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            all_documents.extend(loader.load())\n",
    "        elif file.endswith(\".csv\"):\n",
    "            loader = CSVLoader(file_path)\n",
    "            all_documents.extend(loader.load())\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    split_documents = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=split_documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./knowledge_base_data\"\n",
    "    )\n",
    "    \n",
    "    # Create a retrieval QA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=OpenAI(),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever()\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    kb = build_knowledge_base(\"./documents\")\n",
    "    \n",
    "    # Interactive query loop\n",
    "    while True:\n",
    "        query = input(\"\\nAsk a question (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        response = kb.run(query)\n",
    "        print(\"\\nAnswer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72945ed3-b06d-44cb-8b6d-1ad68929a7f2",
   "metadata": {},
   "source": [
    "To use this system:\n",
    "\n",
    "- Create a directory called documents\n",
    "- Add various files (text, PDF, CSV) containing your personal knowledge\n",
    "- Run the script and ask questions about your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443de1b2-fdad-4f7c-a43d-a18ac8478b27",
   "metadata": {},
   "source": [
    "### Real-life Example: Creating a Customer Support Bot\n",
    "Let's create a support bot for a fictional product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439e23a-4422-40b1-90b1-8b30d61be5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load product documentation\n",
    "loader = TextLoader(\"./data/product_manual.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# Create a retrieval QA system\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# Define the support bot template\n",
    "template = \"\"\"\n",
    "You are a helpful customer support agent for our product. \n",
    "Use the following information to answer the customer's question. \n",
    "If you don't know the answer based on the provided information, \n",
    "say \"I'm not sure about that, but I can connect you with a human agent.\"\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "Customer: {input}\n",
    "AI Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "conversation = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Function to handle customer queries\n",
    "def handle_query(query):\n",
    "    # First try to find information from documentation\n",
    "    try:\n",
    "        doc_answer = qa.run(query)\n",
    "        \n",
    "        # Use the retrieved information to enhance the conversation\n",
    "        enhanced_query = f\"Based on our product information: {doc_answer}\\n\\nCustomer query: {query}\"\n",
    "        response = conversation.predict(input=enhanced_query)\n",
    "    except:\n",
    "        # If retrieval fails, fall back to conversation only\n",
    "        response = conversation.predict(input=query)\n",
    "        \n",
    "    return response\n",
    "\n",
    "# Example interaction\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Customer Support Bot (type 'exit' to quit)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nCustomer: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        response = handle_query(user_input)\n",
    "        print(f\"\\nSupport Agent: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed712d-1de8-4a49-a61e-bb179327916b",
   "metadata": {},
   "source": [
    "For this example, create a product_manual.txt file with information about your product's features, troubleshooting, and FAQs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c02993-95a2-4a57-80a0-bccf80e777d3",
   "metadata": {},
   "source": [
    "### Real-life Example: Data Analysis Assistant\n",
    "Let's create an assistant that can help analyze CSV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03530b1a-51d9-4c81-b98a-889d00fd89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "def create_data_analysis_assistant(csv_file_path):\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Print basic information about the dataset\n",
    "    print(f\"Loaded CSV with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Create a pandas DataFrame agent\n",
    "    agent = create_pandas_dataframe_agent(\n",
    "        llm=OpenAI(temperature=0),\n",
    "        df=df,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = create_data_analysis_assistant(\"./data/sales_data.csv\")\n",
    "    \n",
    "    print(\"\\nData Analysis Assistant Ready!\")\n",
    "    print(\"You can ask questions about your CSV data.\")\n",
    "    print(\"Example: 'What is the total revenue?' or 'Show me trends by month'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nYour question (or type 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            response = agent.run(question)\n",
    "            print(\"\\nResult:\", response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878b868-f10f-41b5-9955-aa6353de5ae5",
   "metadata": {},
   "source": [
    "This assistant can answer questions like:\n",
    "\n",
    "- \"What is the average sales amount?\"\n",
    "- \"Which product category had the highest revenue?\"\n",
    "- \"Show me a breakdown of sales by region\"\n",
    "- \"What was the trend in monthly sales for 2022?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd7142-c1cf-4712-9c65-5bb0f56c417b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
